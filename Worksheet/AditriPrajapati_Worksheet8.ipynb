{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 1: Custom Decision Tree Implementation (Iris Dataset)"
      ],
      "metadata": {
        "id": "ayTdx04-XDC9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n_8ckxNIW4Lz"
      },
      "outputs": [],
      "source": [
        "# Custom Decision Tree Implementation (Iris Dataset)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class CustomDecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        \"\"\"\n",
        "        Initialize the decision tree.\n",
        "        Parameters:\n",
        "        max_depth: int or None\n",
        "            Maximum depth of the tree.\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the decision tree on training data.\n",
        "        \"\"\"\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        \"\"\"\n",
        "        Recursively build the tree based on information gain.\n",
        "        \"\"\"\n",
        "        num_samples, num_features = X.shape\n",
        "        unique_classes = np.unique(y)\n",
        "\n",
        "        # Stopping condition 1: all samples are of same class\n",
        "        if len(unique_classes) == 1:\n",
        "            return {'class': unique_classes[0]}\n",
        "\n",
        "        # Stopping condition 2: reached maximum depth\n",
        "        if num_samples == 0 or (self.max_depth and depth >= self.max_depth):\n",
        "            return {'class': np.bincount(y).argmax()}\n",
        "\n",
        "        best_info_gain = -float('inf')\n",
        "        best_split = None\n",
        "\n",
        "        # Iterate over all features and thresholds\n",
        "        for feature_idx in range(num_features):\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "            for threshold in thresholds:\n",
        "                left_mask = X[:, feature_idx] <= threshold\n",
        "                right_mask = ~left_mask\n",
        "                left_y = y[left_mask]\n",
        "                right_y = y[right_mask]\n",
        "\n",
        "                info_gain = self._information_gain(y, left_y, right_y)\n",
        "                if info_gain > best_info_gain:\n",
        "                    best_info_gain = info_gain\n",
        "                    best_split = {\n",
        "                        'feature_idx': feature_idx,\n",
        "                        'threshold': threshold,\n",
        "                        'left_mask': left_mask,\n",
        "                        'right_mask': right_mask\n",
        "                    }\n",
        "\n",
        "        if best_split is None:\n",
        "            return {'class': np.bincount(y).argmax()}\n",
        "\n",
        "        # Recursively build left and right subtrees\n",
        "        left_tree = self._build_tree(X[best_split['left_mask']], y[best_split['left_mask']], depth + 1)\n",
        "        right_tree = self._build_tree(X[best_split['right_mask']], y[best_split['right_mask']], depth + 1)\n",
        "\n",
        "        return {\n",
        "            'feature_idx': best_split['feature_idx'],\n",
        "            'threshold': best_split['threshold'],\n",
        "            'left_tree': left_tree,\n",
        "            'right_tree': right_tree\n",
        "        }\n",
        "\n",
        "    def _information_gain(self, parent, left, right):\n",
        "        \"\"\"\n",
        "        Calculate information gain of a split.\n",
        "        \"\"\"\n",
        "        parent_entropy = self._entropy(parent)\n",
        "        left_entropy = self._entropy(left)\n",
        "        right_entropy = self._entropy(right)\n",
        "\n",
        "        weighted_avg = (len(left)/len(parent)) * left_entropy + (len(right)/len(parent)) * right_entropy\n",
        "        return parent_entropy - weighted_avg\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        \"\"\"\n",
        "        Calculate entropy of a label set.\n",
        "        \"\"\"\n",
        "        if len(y) == 0:\n",
        "            return 0\n",
        "        probs = np.bincount(y) / len(y)\n",
        "        return -np.sum([p * np.log2(p + 1e-9) for p in probs if p > 0])\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for a dataset.\n",
        "        \"\"\"\n",
        "        return np.array([self._predict_single(x, self.tree) for x in X])\n",
        "\n",
        "    def _predict_single(self, x, tree):\n",
        "        \"\"\"\n",
        "        Predict class label for a single sample.\n",
        "        \"\"\"\n",
        "        if 'class' in tree:\n",
        "            return tree['class']\n",
        "\n",
        "        feature_val = x[tree['feature_idx']]\n",
        "        if feature_val <= tree['threshold']:\n",
        "            return self._predict_single(x, tree['left_tree'])\n",
        "        else:\n",
        "            return self._predict_single(x, tree['right_tree'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 2: Load Iris Dataset and Split"
      ],
      "metadata": {
        "id": "U81EjIuAXON0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Iris Dataset and Split into Train/Test\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset: 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training samples:\", X_train.shape[0])\n",
        "print(\"Test samples:\", X_test.shape[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeLaWn8IXO0D",
        "outputId": "18a9b800-8e76-4868-b31a-39e94dc00000"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 120\n",
            "Test samples: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 3: Train & Evaluate Custom Decision Tree"
      ],
      "metadata": {
        "id": "8_L-ZibgXYGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Evaluate Custom Decision Tree\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize custom decision tree with max_depth=3\n",
        "custom_tree = CustomDecisionTree(max_depth=3)\n",
        "custom_tree.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_custom = custom_tree.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "print(f\"Custom Decision Tree Accuracy: {accuracy_custom:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Kkv9N-fXY3d",
        "outputId": "d591d903-e62f-4513-df3f-de1f216ca652"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Decision Tree Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 4: Train & Evaluate Scikit-learn Decision Tree"
      ],
      "metadata": {
        "id": "9lHdsQ0VXelw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Evaluate Scikit-learn Decision Tree\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize scikit-learn decision tree\n",
        "sklearn_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "sklearn_tree.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_sklearn = sklearn_tree.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "print(f\"Scikit-learn Decision Tree Accuracy: {accuracy_sklearn:.4f}\")\n",
        "\n",
        "# Compare results\n",
        "print(\"\\nAccuracy Comparison:\")\n",
        "print(f\"Custom Decision Tree: {accuracy_custom:.4f}\")\n",
        "print(f\"Scikit-learn Decision Tree: {accuracy_sklearn:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grSuPXrWXgrX",
        "outputId": "79498871-7a55-4f8e-8b38-7d64d38470e5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scikit-learn Decision Tree Accuracy: 1.0000\n",
            "\n",
            "Accuracy Comparison:\n",
            "Custom Decision Tree: 1.0000\n",
            "Scikit-learn Decision Tree: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 5: Ensemble Methods (Random Forest) – Wine Dataset"
      ],
      "metadata": {
        "id": "WupfYfnmYFRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Models: Wine Dataset\n",
        "# Decision Tree & Random Forest\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "wine = load_wine()\n",
        "X_w, y_w = wine.data, wine.target\n",
        "\n",
        "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(X_w, y_w, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train_w, y_train_w)\n",
        "y_pred_dt = dt_classifier.predict(X_test_w)\n",
        "f1_dt = f1_score(y_test_w, y_pred_dt, average='weighted')\n",
        "print(f\"Decision Tree F1 Score: {f1_dt:.4f}\")\n",
        "\n",
        "# Random Forest\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "rf_classifier.fit(X_train_w, y_train_w)\n",
        "y_pred_rf = rf_classifier.predict(X_test_w)\n",
        "f1_rf = f1_score(y_test_w, y_pred_rf, average='weighted')\n",
        "print(f\"Random Forest F1 Score: {f1_rf:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcTJW9pqYF8M",
        "outputId": "15f30937-ed88-419d-b7b7-0763254d6a0f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree F1 Score: 0.9440\n",
            "Random Forest F1 Score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 6: Hyperparameter Tuning – Random Forest Classifier (GridSearchCV)"
      ],
      "metadata": {
        "id": "dPZz7IiwYRCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning: Random Forest Classifier\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define hyperparameter grid (correct for scikit-learn >=1.1)\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1_weighted',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train_w, y_train_w)\n",
        "\n",
        "# Best parameters and evaluation\n",
        "print(\"Best Parameters (Random Forest Classifier):\", grid_search.best_params_)\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred_best_rf = best_rf.predict(X_test_w)\n",
        "f1_best_rf = f1_score(y_test_w, y_pred_best_rf, average='weighted')\n",
        "print(f\"Best Random Forest F1 Score: {f1_best_rf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpsWdHhVYR-e",
        "outputId": "b068701f-18aa-43e6-96aa-8c74a45755a4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters (Random Forest Classifier): {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Best Random Forest F1 Score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 7: Regression with Decision Tree and Random Forest"
      ],
      "metadata": {
        "id": "JEwqJ3pfZN25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Regressor and Random Forest Regressor\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# Load California Housing dataset from OpenML\n",
        "housing = fetch_openml(name=\"california_housing\", version=1, as_frame=True)\n",
        "\n",
        "# Features and target\n",
        "X = housing.data.copy()          # DataFrame of features\n",
        "y = housing.target.copy().astype(float)  # Target as float\n",
        "\n",
        "# Combine into one DataFrame for feature engineering\n",
        "df = X.copy()\n",
        "df[\"MedHouseVal\"] = y  # Explicitly name target column\n",
        "\n",
        "# Feature Engineering: Create average-based features\n",
        "df[\"AveRooms\"]  = df[\"total_rooms\"] / df[\"households\"]\n",
        "df[\"AveBedrms\"] = df[\"total_bedrooms\"] / df[\"households\"]\n",
        "df[\"AveOccup\"]  = df[\"population\"] / df[\"households\"]\n",
        "\n",
        "# Select relevant features for modeling\n",
        "df = df[\n",
        "    [\n",
        "        \"median_income\",\n",
        "        \"housing_median_age\",\n",
        "        \"AveRooms\",\n",
        "        \"AveBedrms\",\n",
        "        \"population\",\n",
        "        \"AveOccup\",\n",
        "        \"latitude\",\n",
        "        \"longitude\",\n",
        "        \"MedHouseVal\"\n",
        "    ]\n",
        "]\n",
        "\n",
        "# Remove missing values if any\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Split features and target\n",
        "X_r = df.drop(\"MedHouseVal\", axis=1).astype(float)\n",
        "y_r = df[\"MedHouseVal\"].astype(float)\n",
        "\n",
        "# Train-test split (80% train, 20% test)\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
        "    X_r, y_r, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train_r, y_train_r)\n",
        "y_pred_dt = dt_regressor.predict(X_test_r)\n",
        "mse_dt = mean_squared_error(y_test_r, y_pred_dt)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(random_state=42)\n",
        "rf_regressor.fit(X_train_r, y_train_r)\n",
        "y_pred_rf = rf_regressor.predict(X_test_r)\n",
        "mse_rf = mean_squared_error(y_test_r, y_pred_rf)\n",
        "\n",
        "print(\"Regression Models Evaluation:\")\n",
        "print(f\"Decision Tree Regressor MSE: {mse_dt:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c5X5dHUdOC0",
        "outputId": "a2e943ce-9471-4db0-98ae-aca3b9fac3b2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression Models Evaluation:\n",
            "Decision Tree Regressor MSE: 5499044142.5924\n",
            "Random Forest Regressor MSE: 2683756555.1180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLOCK 8: Hyperparameter Tuning (Random Forest Regressor)"
      ],
      "metadata": {
        "id": "W5f-yt9Fhopn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using RandomizedSearchCV\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV for Random Forest Regressor\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=RandomForestRegressor(random_state=42),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,            # number of random combinations to try\n",
        "    cv=5,                 # 5-fold cross-validation\n",
        "    scoring='neg_mean_squared_error',  # minimize MSE\n",
        "    random_state=42,\n",
        "    n_jobs=-1             # use all processors\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV\n",
        "random_search.fit(X_train_r, y_train_r)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params_rf = random_search.best_params_\n",
        "best_rf_reg = random_search.best_estimator_\n",
        "\n",
        "# Predict using the best Random Forest Regressor\n",
        "y_pred_best_rf = best_rf_reg.predict(X_test_r)\n",
        "mse_best_rf = mean_squared_error(y_test_r, y_pred_best_rf)\n",
        "\n",
        "print(\"Random Forest Regressor Hyperparameter Tuning Results:\")\n",
        "print(f\"Best Hyperparameters: {best_params_rf}\")\n",
        "print(f\"Best Random Forest Regressor MSE on Test Set: {mse_best_rf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqggLZA6hqbW",
        "outputId": "65ba9d3b-350b-4047-a044-c6c0b4186f55"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Regressor Hyperparameter Tuning Results:\n",
            "Best Hyperparameters: {'n_estimators': 100, 'min_samples_split': 5, 'max_depth': 30}\n",
            "Best Random Forest Regressor MSE on Test Set: 2686814150.2137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Points :\n",
        "\n",
        "* Decision Trees split data using feature-based thresholds and can be implemented both from scratch and using Scikit-learn.\n",
        "\n",
        "* Scikit-learn’s Decision Tree performs better due to optimized algorithms and pruning.\n",
        "\n",
        "* Random Forest is an ensemble method that combines multiple decision trees to improve accuracy and reduce overfitting.\n",
        "\n",
        "* F1 Score is an effective metric for multi-class classification as it balances precision and recall.\n",
        "\n",
        "* GridSearchCV performs exhaustive hyperparameter tuning using cross-validation for classification models.\n",
        "\n",
        "* RandomizedSearchCV is computationally efficient for tuning regression models with large datasets.\n",
        "\n",
        "* In regression, Random Forest Regressor achieves lower MSE than Decision Tree Regressor due to ensemble averaging.\n",
        "\n",
        "* Large MSE values occur because the target variable is large and errors are squared.\n",
        "\n",
        "* Feature scaling is not required for tree-based models since they use threshold comparisons, not distance metrics.\n",
        "\n",
        "* Hyperparameter tuning improves model generalization and performance stability."
      ],
      "metadata": {
        "id": "He8iKysBlIh6"
      }
    }
  ]
}